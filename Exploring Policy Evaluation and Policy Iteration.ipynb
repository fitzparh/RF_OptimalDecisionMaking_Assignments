{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation vs. Policy Iteration\n",
    "\n",
    "While there is understandably some confusion between policy evaluation and policy iteration given the closeness of names, at a high level the distinction is quite simple: policy evaluation calculates the value function for a given policy (set of state action pairs) while policy iteration tries to find an **improvement on that given policy**. The two processes are repeated iteratively until policy converges i.e. if the agent cannot choose a better policy, the policy has converged and the agent has found the optimal policy. \n",
    "\n",
    "To further explore these concepts, I'm going to explore code previously created for solving the 'gridworld problem.' In addition to identifying how many 'cycles' of policy evaluation/iteration the agent must go through before policy convergence, I will also test different values for gamma to see what, if any change it has on determining the optimal policy. \n",
    "\n",
    "### Policy Iteration process\n",
    "Before I begin, below are the 3 main steps taking place in the policy iteration code\n",
    "*Note: Gridworld is deterministic, leading to the same new state given each state and action\n",
    "\n",
    "1. Randomly initialize V(s) and the p(s)\n",
    "2. Perform policy evalution to find the current V(s)\n",
    "3. Perform policy improvement (find a new (improved) policy based on the previously calculated value function)\n",
    "\n",
    "\n",
    "Let's run the code as is to compare the initial policy to the found optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  U  |  U  |  U  |     |\n",
      "---------------------------\n",
      "  R  |     |  L  |     |\n",
      "---------------------------\n",
      "  D  |  U  |  L  |  R  |\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence - will break out when policy does not change\n",
    "  while True:\n",
    "    # policy evaluation step - we already know how to do this!\n",
    "    while True:\n",
    "      \n",
    "      biggest_change = 0\n",
    "      for s in states:\n",
    "        old_v = V[s]\n",
    "\n",
    "        # V(s) only has value if it's not a terminal state\n",
    "        if s in policy:\n",
    "          a = policy[s]\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          V[s] = r + GAMMA * V[grid.current_state()]\n",
    "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "      if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "\n",
    "    # policy improvement step\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "      if s in policy:\n",
    "        old_a = policy[s]\n",
    "        new_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > best_value:\n",
    "            best_value = v\n",
    "            new_a = a\n",
    "        policy[s] = new_a\n",
    "        if new_a != old_a:\n",
    "          is_policy_converged = False\n",
    "\n",
    "    if is_policy_converged:\n",
    "      break\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial policy, if when we right below the wall, the policy told the agent to go left. Conversely, in the optimal policy we went right - this makes sense given the optimal policy takes into account how each step costs a little bit of reward. Now, I'm curious to see how many cycles of policy evaluation/iteration took place. To do this, I'm going to add a counter under the first while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  R  |  R  |  U  |     |\n",
      "---------------------------\n",
      "  D  |     |  D  |     |\n",
      "---------------------------\n",
      "  U  |  D  |  U  |  D  |\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence - will break out when policy does not change\n",
    "  count = 0 # set count to zero\n",
    "  while True:\n",
    "    count += 1 #increment for every cycle\n",
    "    while True:\n",
    "      \n",
    "      biggest_change = 0\n",
    "      for s in states:\n",
    "        old_v = V[s]\n",
    "\n",
    "        # V(s) only has value if it's not a terminal state\n",
    "        if s in policy:\n",
    "          a = policy[s]\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          V[s] = r + GAMMA * V[grid.current_state()]\n",
    "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "      if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "\n",
    "    # policy improvement step\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "      if s in policy:\n",
    "        old_a = policy[s]\n",
    "        new_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > best_value:\n",
    "            best_value = v\n",
    "            new_a = a\n",
    "        policy[s] = new_a\n",
    "        if new_a != old_a:\n",
    "          is_policy_converged = False\n",
    "\n",
    "    if is_policy_converged:\n",
    "      break\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "  print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it can change each time the code is run (I've gotten 4,5,6), this time the policy was updated 4 times before convergence\n",
    "\n",
    "## Changing Gamma\n",
    "\n",
    "Next, I'm going to decrease gamma, thereby increasing the discount and imposing more importance on acheiving the immediate reward vs. future rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  R  |  L  |  U  |     |\n",
      "---------------------------\n",
      "  R  |     |  L  |     |\n",
      "---------------------------\n",
      "  U  |  U  |  L  |  D  |\n",
      "values:\n",
      "---------------------------\n",
      " 0.00| 0.05| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.05| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.05\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence - will break out when policy does not change\n",
    "  while True:\n",
    "    # policy evaluation step - we already know how to do this!\n",
    "    while True:\n",
    "      \n",
    "      biggest_change = 0\n",
    "      for s in states:\n",
    "        old_v = V[s]\n",
    "\n",
    "        # V(s) only has value if it's not a terminal state\n",
    "        if s in policy:\n",
    "          a = policy[s]\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          V[s] = r + GAMMA * V[grid.current_state()]\n",
    "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "      if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "\n",
    "    # policy improvement step\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "      if s in policy:\n",
    "        old_a = policy[s]\n",
    "        new_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > best_value:\n",
    "            best_value = v\n",
    "            new_a = a\n",
    "        policy[s] = new_a\n",
    "        if new_a != old_a:\n",
    "          is_policy_converged = False\n",
    "\n",
    "    if is_policy_converged:\n",
    "      break\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After decreasing gamma from .9 to .05, we can see there is a noticable difference between values as we move farther away from the terminal state. When gamma was set to .9, the value decreased by 10% when moving away from the terminal state. When gamma was set to .05, the values decreased by 95%"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
