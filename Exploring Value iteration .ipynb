{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration vs. Policy Iteration\n",
    "\n",
    "While both policy iteration and value iteration can be used to solve the control problem, value iteration is often cited as a more efficient technique. This is mainly due to the fact that value iteration combines policy evaluation and policy improvement into one step. More specifically, policy iteration requires that we evaluate policy until convergence before doing a single policy improvement step whereas value iteration *only* requires one iteration of policy evaluation before moving to the policy improvement phase. Furthermore, with value iteration we don't need to wait for the entire kth iteration of V to be available before we start calculating the (k+1)th iteration of V. Rather, we can just update it \"in place.\" Since policy improvement uses **argmax**, by taking the **max**, we're just doing the next policy evaluation step without calculating policy explicity. \n",
    "\n",
    "In the most simplistic terms, the steps for value iteration can be summed up as follows:\n",
    "\n",
    "1. Initialize all V0(s) = 0 for all states\n",
    "2. Compute the value of V1(s) given all the values of V0(s)\n",
    "2. Repeat until the values don't change i.e. convergence is reached\n",
    "\n",
    "## Value iteration in deterministic Grid World environment\n",
    "\n",
    "To further illustrate value iteration concepts, I'm going to again explore the code previously used in the 'gridworld problem.' Similiar to my experiements with policy iteration, I will also test different values for gamma to see what, if any change it has on determining the optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  L  |  R  |  D  |     |\n",
      "---------------------------\n",
      "  L  |     |  U  |     |\n",
      "---------------------------\n",
      "  L  |  D  |  L  |  D  |\n",
      "1 0.733894342227\n",
      "2 0.254122076696\n",
      "3 0.0737801756191\n",
      "4 0.0079215805718\n",
      "5 0\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence\n",
    "  # V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "  count = 0\n",
    "  while True:\n",
    "    count += 1\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        new_v = float('-inf')\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > new_v:\n",
    "            new_v = v\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "        \n",
    "    print(count,biggest_change)\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "\n",
    "  # find a policy that leads to optimal value function\n",
    "  for s in policy.keys():\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "    # loop through all possible actions to find the best current action\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      grid.set_state(s)\n",
    "      r = grid.move(a)\n",
    "      v = r + GAMMA * V[grid.current_state()]\n",
    "      if v > best_value:\n",
    "        best_value = v\n",
    "        best_a = a\n",
    "    policy[s] = best_a\n",
    "\n",
    "  # our goal here is to verify that we get the same answer as with policy iteration\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "  print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking specifically at the value iteration loop, it looks very similar to policy evaluation except now we're looping through all the possible actions and taking the maximum value. It breaks when the biggest change is below the 'small enough' threshold. Next, we take the optimal value function and find the optimal policy (we loop through all the actions and find the action that gives us the best future reward.\n",
    "\n",
    "Based on the output/resutls, it looks like we get the same values as the policy iteration code. It looks like values did not change after 4 iterations. \n",
    "\n",
    "## Changing Gamma\n",
    "Next, I'm going to decrease gamma, thereby increasing the discount and imposing more importance on acheiving the immediate reward vs. future rewards. Let's see if we get the same results as with policy iteration i.e. changing gamma to .05 led to 95% decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "initial policy:\n",
      "---------------------------\n",
      "  D  |  R  |  L  |     |\n",
      "---------------------------\n",
      "  U  |     |  L  |     |\n",
      "---------------------------\n",
      "  R  |  L  |  R  |  D  |\n",
      "1 0.872098210339\n",
      "2 0.043604910517\n",
      "3 0.00216999529037\n",
      "4 6.170682481e-05\n",
      "values:\n",
      "---------------------------\n",
      " 0.00| 0.05| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.05| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.05\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# this is deterministic\n",
    "# all p(s',r|s,a) = 1 or 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "  # we want to see if this will encourage finding a shorter path to the goal\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # we'll randomly choose an action and update as we learn\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initial policy\n",
    "  print(\"initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # initialize V(s)\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "      V[s] = np.random.random()\n",
    "    else:\n",
    "      # terminal state\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence\n",
    "  # V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "  count = 0\n",
    "  while True:\n",
    "    count += 1\n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "\n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        new_v = float('-inf')\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > new_v:\n",
    "            new_v = v\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "        \n",
    "    print(count,biggest_change)\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "\n",
    "  # find a policy that leads to optimal value function\n",
    "  for s in policy.keys():\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "    # loop through all possible actions to find the best current action\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      grid.set_state(s)\n",
    "      r = grid.move(a)\n",
    "      v = r + GAMMA * V[grid.current_state()]\n",
    "      if v > best_value:\n",
    "        best_value = v\n",
    "        best_a = a\n",
    "    policy[s] = best_a\n",
    "\n",
    "  # our goal here is to verify that we get the same answer as with policy iteration\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "  print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, with value iteration we also saw a 95% decrease when moving away from the terminal state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
