{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning: Introduction\n",
    "\n",
    "Often viewed as one of the most important techniques in reinforcement learning due to its applicability to continuous (non-epsiodic) tasks, Temporal Difference Learning (TD) combines ideas from Dynamic Programming and Monte Carlo methods. Similar to Monte Carlo (MC), TD learning does not require a model of the learning enviornment; the method learns directly from the raw experience in a partially unknown system with each recorded sample. But contrary to MC, TD does not need to wait for an episode to conclude, before it starts updating value estimates. \n",
    "\n",
    "## TD: Using the recursive definition\n",
    "\n",
    "As mentioned in my previous write up \"Exploring Monte Carlo,\" MC allows us to calculate return averages and identify the value function in different ways. More specifically, we don't have to store all returns in a list and have the ability to define the value function recursively. TD builds on these elements of MC by replacing the return in the update equation with the recursive definition of V. Instead of calculating G, the full return, TD uses V(s) for the next state. \n",
    "\n",
    "## Sources of randomness - where estimates come from: MC vs TD\n",
    "\n",
    "In MC, randomness came from the fact that each episode could play out in a different way due to stochastic policy or stochastic state transistions. The return for a state therefore would be different if all the later state transitions had some randomness. With TD, another source of randomness is in play: it uses Reward plus gamma times V(s)prime to estimate the return G. \n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "When talking about Temporal Difference learning, chances are the conversation will eventually move towards the topic of Q-Learning. While Q-Learning is a temporal difference alogrithim, it is unique in that it deviates from generalized policy iteration. With generalized policy iteration, you alternate between policy evaluation and policy improvement which means choosing an action greedily based on the current value estimate. In other words, actions are in line with the current best policy.  In contrast, the actions you take in Q-learning can be completely random. Still, with the randomness in Q-Learning, you can still find the optimal policy. \n",
    "\n",
    "Let's see Q-learning in action using the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0kt8n+BKZGzcP8wzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23fc5befba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.26| 0.05| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.13| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.28| 0.07| 0.05| 0.11|\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "  # suboptimal policies\n",
    "  # e.g.\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   R* |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   R  |   U  |   L  |\n",
    "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "  # on right.\n",
    "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "  #\n",
    "  # grid = standard_grid()\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "  # initialize Q(s,a)\n",
    "  Q = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0\n",
    "\n",
    "  # let's also keep track of how many times Q[s] has been updated\n",
    "  update_counts = {}\n",
    "  update_counts_sa = {}\n",
    "  for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      update_counts_sa[s][a] = 1.0\n",
    "\n",
    "  # repeat until convergence\n",
    "  t = 1.0\n",
    "  deltas = []\n",
    "  for it in range(10000):\n",
    "    if it % 100 == 0:\n",
    "      t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "      print(\"it:\", it)\n",
    "\n",
    "    # instead of 'generating' an epsiode, we will PLAY\n",
    "    # an episode within this loop\n",
    "    s = (2, 0) # start state\n",
    "    grid.set_state(s)\n",
    "\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    a, _ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    count = 0\n",
    "    while not grid.game_over():\n",
    "      count += 1  \n",
    "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "      # random action also works, but slower since you can bump into walls\n",
    "      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "      r = grid.move(a)\n",
    "      s2 = grid.current_state()\n",
    "\n",
    "      # adaptive learning rate\n",
    "      alpha = ALPHA / update_counts_sa[s][a]\n",
    "      update_counts_sa[s][a] += 0.005\n",
    "\n",
    "      # we will update Q(s,a) AS we experience the episode\n",
    "      old_qsa = Q[s][a]\n",
    "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "      # we will use this max[a']{ Q(s',a')} in our update\n",
    "      # even if we do not end up taking this action in the next step\n",
    "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "      # we would like to know how often Q(s) has been updated too\n",
    "      update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "      # next state becomes current state\n",
    "      s = s2\n",
    "     \n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # determine the policy from Q*\n",
    "  # find V* from Q*\n",
    "  policy = {}\n",
    "  V = {}\n",
    "  for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "  # what's the proportion of time we spend updating each part of Q?\n",
    "  print(\"update counts:\")\n",
    "  total = np.sum(list(update_counts.values()))\n",
    "  for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "  print_values(update_counts, grid)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away, we can spot the difference between the optimal policy derived from Q-Learning vs. Policy evaluation/iteration approaches e.g. (2,0) states an 'Up' move vs. to the right. Of course, the output might differ next time given there is now a degree of randomness involved. I did notice that, the time it took to converge did take longer (testament to the fact that with Q-learning the agent will act sub-optimally very often). Granted, we went through a 1000 iterations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
